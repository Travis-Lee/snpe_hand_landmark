<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Snapdragon Neural Processing Engine SDK: Running the Inception v3 Model</title>
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" ></link>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Snapdragon Neural Processing Engine SDK
   <span id="projectnumber"></span></div>
   <div id="projectbrief">Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('tutorial_inceptionv3.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Running the Inception v3 Model </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="overview_native_app"></a>
Overview</h1>
<p>The example C++ application in this tutorial is called <b>snpe-net-run</b>. It is a command line executable that executes a neural network using SNPE SDK APIs.</p>
<p>The required arguments to snpe-net-run are:</p><ul>
<li>A neural network model in the DLC file format</li>
<li>An input list file with paths to the input data.</li>
</ul>
<p>Optional arguments to snpe-net-run are:</p><ul>
<li>Choice of GPU, DSP or AIP runtimes (default is CPU)</li>
<li>Output directory (default is ./output)</li>
<li>Show help description</li>
</ul>
<p>snpe-net-run creates and populates an output directory with the results of executing the neural network on the input data.</p>
<p><a class="anchor" id="fig_neural_network"></a>  <div style= text-align:left;><img src="images/neural_network.png" alt="SNPE" width="40%" height="40%"><br/><br/><b></b></div><br/> </p>
<p>The SNPE SDK provides Linux and Android binaries of <b>snpe-net-run</b> under</p><ul>
<li>$SNPE_ROOT/bin/x86_64-linux-clang</li>
<li>$SNPE_ROOT/bin/arm-android-clang6.0</li>
<li>$SNPE_ROOT/bin/aarch64-android-clang6.0</li>
<li>$SNPE_ROOT/bin/aarch64-linux-gcc4.9</li>
<li>$SNPE_ROOT/bin/arm-linux-gcc4.9sf</li>
<li>$SNPE_ROOT/bin/aarch64-oe-linux-gcc6.4</li>
<li>$SNPE_ROOT/bin/arm-oe-linux-gcc6.4hf</li>
</ul>
<h1><a class="anchor" id="tutorial_inceptionv3_prerequisites"></a>
Prerequisites</h1>
<ul>
<li>
The SNPE SDK has been set up following the <a class="el" href="setup.html">SNPE Setup</a> chapter. </li>
<li>
The <a class="el" href="tutorial_setup.html">Tutorials Setup</a> has been completed. </li>
<li>
TensorFlow is installed (see <a class="el" href="setup_tensorflow.html">TensorFlow Setup</a>) </li>
</ul>
<h1><a class="anchor" id="tutorial_inceptionv3_introduction"></a>
Introduction</h1>
<p>The Inception v3 Imagenet classification model is trained to classify images with 1000 labels.</p>
<p>The examples below shows the steps required to execute a pretrained <b>optimized</b> and optionally <b>quantized</b> Inception v3 model with <b>snpe-net-run</b> to classify a set of sample images. An optimized and quantized model is used in this example to showcase the DSP and AIP runtimes which execute quantized 8-bit neural network models.</p>
<p>The DLC for the model used in this tutorial was generated and optimized using the TensorFlow optimizer tool, during the <a class="el" href="tutorial_setup.html#tutorial_setup_inception_v3">Getting Inception v3</a> portion of the <a class="el" href="tutorial_setup.html">Tutorials Setup</a>, by the script $SNPE_ROOT/models/inception_v3/scripts/setup_inceptionv3.py. Additionally, if a fixed-point runtime such as DSP or AIP was selected when running the setup script, the model was quantized by snpe-dlc-quantize.</p>
<p><a class="el" href="quantized_models.html">Learn more about a quantized model</a>.</p>
<h1><a class="anchor" id="tutorial_inceptionv3_run_on_linux_host"></a>
Run on Linux Host</h1>
<p>Go to the base location for the model and run <b>snpe-net-run</b></p>
<pre class="fragment">cd $SNPE_ROOT/models/inception_v3
snpe-net-run --container dlc/inception_v3_quantized.dlc --input_list data/cropped/raw_list.txt
</pre><p>After snpe-net-run completes, the results are populated in the $SNPE_ROOT/models/inception_v3/output directory. There should be one or more .log files and several Result_X directories, each containing a <b>softmax:0.raw</b> file.</p>
<p>One of the inputs is data/cropped/handicap_sign.raw and it was created from data/cropped/handicap_sign.jpg which looks like the following:</p>
<p><a class="anchor" id="fig_data_ex0_1"></a>  <div style= text-align:left;><img src="images/handicap_sign.jpg" alt="SNPE" width="30%" height="30%"><br/><br/><b></b></div><br/> </p>
<p>With this input, snpe-net-run created the output file $SNPE_ROOT/models/inception_v3/output/Result_0/softmax:0.raw. It holds the output tensor data of 1000 probabilities for the 1000 categories. The element with the highest value represents the top classification. A python script to interpret the classification results is provided and can be used as follows:</p>
<pre class="fragment">python3 $SNPE_ROOT/models/inception_v3/scripts/show_inceptionv3_classifications.py -i data/cropped/raw_list.txt \
                                                                                  -o output/ \
                                                                                  -l data/imagenet_slim_labels.txt
</pre><p>The output should look like the following, showing classification results for all the images.</p>
<pre class="fragment">Classification results
&lt;input_files_dir&gt;/trash_bin.raw     0.850245 413 ashcan
&lt;input_files_dir&gt;/plastic_cup.raw   0.972899 648 measuring cup
&lt;input_files_dir&gt;/chairs.raw        0.286483 832 studio couch
&lt;input_files_dir&gt;/handicap_sign.raw 0.430206 920 street sign
&lt;input_files_dir&gt;/notice_sign.raw   0.138858 459 brass
</pre><p><b>Note:</b> The &lt;input_files_dir&gt; above maps to a path such as $SNPE_ROOT/models/inception_v3/data/cropped/</p>
<p>The output shows the image was classified as "street sign" (index 932 of the labels) with a probability of 0.383475. The rest of the output can be examined to see the model's classification on other images.</p>
<p><b>Binary data input</b></p>
<p>Note that the Inception v3 image classification model does not accept jpg files as input. The model expects its input tensor dimension to be 299x299x3 as a float array. The scripts/setup_inception_v3.py script performs a jpg to binary data conversion by calling scripts/create_inception_v3_raws.py. The scripts are an example of how jpg images can be preprocessed to generate input for the Inception v3 model.</p>
<h1><a class="anchor" id="tutorial_inceptionv3_building_and_running_on_android"></a>
Run on Android Target</h1>
<p><b>Select target architecture</b></p>
<p>SNPE provides Android binaries for armeabi-v7a and arm64-v8a architectures. For each architecture, there are binaries compiled with clang6.0 using libc++ STL implementation. The following shows the commands to select the desired binaries:</p>
<pre class="fragment"># architecture: armeabi-v7a - compiler: clang - STL: libc++
export SNPE_TARGET_ARCH=arm-android-clang6.0
export SNPE_TARGET_STL=libc++_shared.so

# architecture: arm64-v8a - compiler: clang - STL: libc++
export SNPE_TARGET_ARCH=aarch64-android-clang6.0
export SNPE_TARGET_STL=libc++_shared.so
</pre><p>For simplicity, this tutorial sets the target binaries to arm-android-clang6.0, which use libc++_shared.so, for commands on host and on target.</p>
<p><b>Push binaries to target</b></p>
<p>Push SNPE libraries and the prebuilt snpe-net-run executable to /data/local/tmp/snpeexample on the Android target.</p>
<pre class="fragment">export SNPE_TARGET_ARCH=arm-android-clang6.0
export SNPE_TARGET_STL=libc++_shared.so

adb shell "mkdir -p /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin"
adb shell "mkdir -p /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib"
adb shell "mkdir -p /data/local/tmp/snpeexample/dsp/lib"

adb push $SNPE_ROOT/lib/$SNPE_TARGET_ARCH/$SNPE_TARGET_STL \
      /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
adb push $SNPE_ROOT/lib/$SNPE_TARGET_ARCH/*.so \
      /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
adb push $SNPE_ROOT/lib/dsp/*.so \
      /data/local/tmp/snpeexample/dsp/lib
adb push $SNPE_ROOT/bin/$SNPE_TARGET_ARCH/snpe-net-run \
      /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
</pre><p><b>Set up enviroment variables</b></p>
<p>Set up the library path, the path variable, and the target architecture in adb shell to run the executable with the -h argument to see its description.</p>
<pre class="fragment">adb shell
export SNPE_TARGET_ARCH=arm-android-clang6.0
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
export PATH=$PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
snpe-net-run -h
exit
</pre><p><b>Push model data to Android target</b></p>
<p>To execute the Inception v3 classification model on Android target follow these steps:</p>
<pre class="fragment">cd $SNPE_ROOT/models/inception_v3
mkdir data/rawfiles &amp;&amp; cp data/cropped/*.raw data/rawfiles/
adb shell "mkdir -p /data/local/tmp/inception_v3"
adb push data/rawfiles /data/local/tmp/inception_v3/cropped
adb push data/target_raw_list.txt /data/local/tmp/inception_v3
adb push dlc/inception_v3_quantized.dlc /data/local/tmp/inception_v3
rm -rf data/rawfiles
</pre><p><b>Note:</b> It may take some time to push the Inception v3 dlc file to the target.</p>
<h1><a class="anchor" id="tutorial_inceptionv3_running_on_android_cpu"></a>
Running on Android using CPU Runtime</h1>
<p>The Android C++ executable is run with the following commands:</p>
<pre class="fragment">adb shell
export SNPE_TARGET_ARCH=arm-android-clang6.0
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
export PATH=$PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
cd /data/local/tmp/inception_v3
snpe-net-run --container inception_v3_quantized.dlc --input_list target_raw_list.txt
exit
</pre><p>The executable will create the results folder: /data/local/tmp/inception_v3/output. To pull the output:</p>
<pre class="fragment">adb pull /data/local/tmp/inception_v3/output output_android
</pre><p>Check the classification results by running the following python script:</p>
<pre class="fragment">python3 scripts/show_inceptionv3_classifications.py -i data/target_raw_list.txt \
                                                   -o output_android/ \
                                                   -l data/imagenet_slim_labels.txt
</pre><p>The output should look like the following, showing classification results for all the images.</p>
<pre class="fragment">Classification results
cropped/trash_bin.raw     0.850245 413 ashcan
cropped/plastic_cup.raw   0.972899 648 measuring cup
cropped/chairs.raw        0.286483 832 studio couch
cropped/handicap_sign.raw 0.430207 920 street sign
cropped/notice_sign.raw   0.138857 459 brass
</pre><h1><a class="anchor" id="tutorial_inceptionv3_running_on_android_dsp"></a>
Running on Android using DSP Runtime</h1>
<p>Try running on an Android target with the --use_dsp option as follows: <br />
Note the extra environment variable ADSP_LIBRARY_PATH must be set to use DSP. (See <a class="el" href="dsp_runtime.html">DSP Runtime Environment</a> for details.)</p>
<pre class="fragment">adb shell
export SNPE_TARGET_ARCH=arm-android-clang6.0
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
export PATH=$PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
export ADSP_LIBRARY_PATH="/data/local/tmp/snpeexample/dsp/lib;/system/lib/rfsa/adsp;/system/vendor/lib/rfsa/adsp;/dsp"
cd /data/local/tmp/inception_v3
snpe-net-run --container inception_v3_quantized.dlc --input_list target_raw_list.txt --use_dsp
exit
</pre><p>Pull the output into an output_android_dsp directory.</p>
<pre class="fragment">adb pull /data/local/tmp/inception_v3/output output_android_dsp
</pre><p>Check the classification results by running the following python script:</p>
<pre class="fragment">python3 scripts/show_inceptionv3_classifications.py -i data/target_raw_list.txt \
                                                   -o output_android_dsp/ \
                                                   -l data/imagenet_slim_labels.txt
</pre><p>The output should look like the following, showing classification results for all the images.</p>
<pre class="fragment">Classification results
cropped/trash_bin.raw     0.639935 413 ashcan
cropped/plastic_cup.raw   0.937354 648 measuring cup
cropped/chairs.raw        0.275142 832 studio couch
cropped/handicap_sign.raw 0.134832 920 street sign
cropped/notice_sign.raw   0.258279 459 brass
</pre><p>Classification results are identical to the run with CPU runtime, but there are differences in the probabilities associated with the output labels due to floating point precision differences.</p>
<h1><a class="anchor" id="tutorial_inceptionv3_running_on_android_aip"></a>
Running on Android using AIP Runtime</h1>
<p>The AIP runtime allows you to run the Inception v3 model on the HTA. <br />
Running the model using the AIP runtime requires setting the --runtime argument as 'aip' in the script $SNPE_ROOT/models/inception_v3/scripts/setup_inceptionv3.py to allow HTA-specific metadata to be packed into the DLC that is required by the AIP runtime. <br />
Refer to <a class="el" href="tutorial_setup.html#tutorial_setup_inception_v3">Getting Inception v3</a> for more details.</p>
<p>Other than that the additional settings for AIP runtime are quite similar to those for the DSP runtime.</p>
<p>Try running on an Android target with the --use_aip option as follows: <br />
Note the extra environment variable ADSP_LIBRARY_PATH must be set to use DSP. (See <a class="el" href="dsp_runtime.html">DSP Runtime Environment</a> for details.)</p>
<pre class="fragment">adb shell
export SNPE_TARGET_ARCH=arm-android-clang6.0
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
export PATH=$PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
export ADSP_LIBRARY_PATH="/data/local/tmp/snpeexample/dsp/lib;/system/lib/rfsa/adsp;/system/vendor/lib/rfsa/adsp;/dsp"
cd /data/local/tmp/inception_v3
snpe-net-run --container inception_v3_quantized.dlc --input_list target_raw_list.txt --use_aip
exit
</pre><p>Pull the output into an output_android_aip directory.</p>
<pre class="fragment">adb pull /data/local/tmp/inception_v3/output output_android_aip
</pre><p>Check the classification results by running the following python script:</p>
<pre class="fragment">python scripts/show_inceptionv3_classifications.py -i data/target_raw_list.txt \
                                                   -o output_android_aip/ \
                                                   -l data/imagenet_slim_labels.txt
</pre><p>The output should look like the following, showing classification results for all the images.</p>
<pre class="fragment">Classification results
cropped/trash_bin.raw     0.683813 413 ashcan
cropped/plastic_cup.raw   0.971473 648 measuring cup
cropped/chairs.raw        0.429178 832 studio couch
cropped/handicap_sign.raw 0.338605 920 street sign
cropped/notice_sign.raw   0.154364 459 brass
</pre><p>Classification results are identical to the run with CPU runtime, but there are differences in the probabilities associated with the output labels due to floating point precision differences. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 -->
<!-- start footer part -->
<div id="nav-path" class="navpath" font-size:small;><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
      <p align="right">
        80-NL315-14 A <br>
        MAY CONTAIN U.S. AND INTERNATIONAL EXPORT CONTROLLED INFORMATION
        <!--If the Controlled Distribution statement is to be included, uncomment below:-->
        <!--<b>Controlled Distribution - DO NOT COPY</b>-->
        <img class="footer" width:5%; alt="QTI Logo" src="images/QTI_Logo.png" />
      </p>
    </li>
  </ul>
</div>
</body>
</html>
