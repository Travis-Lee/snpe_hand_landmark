<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Snapdragon Neural Processing Engine SDK: TensorFlow Network Models</title>
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" ></link>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Snapdragon Neural Processing Engine SDK
   <span id="projectnumber"></span></div>
   <div id="projectbrief">Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('tensorflow_models.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">TensorFlow Network Models </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="tensorflow_converter"></a>
Converting Models from TensorFlow to SNPE</h1>
<p>This chapter covers the conversion tools used to convert a TensorFlow model to a SNPE DLC file. The DLC file is used by the SNPE runtime for execution of the neural network.</p>
<p><b>Prerequisites</b> </p><ul>
<li>
A working TensorFlow setup. Follow TensorFlow setup instructions included at <a class="el" href="setup_tensorflow.html#get_tensorflow">TensorFlow Setup</a>. </li>
<li>
<p class="startli">A trained TensorFlow model saved as either: </p><ul>
<li>
A frozen TensorFlow model (pb file) </li>
<li>
A pair of checkpoint and graph meta files  </li>
</ul>
<p class="endli"></p>
</li>
</ul>
<h2><a class="anchor" id="graph_requirements"></a>
TensorFlow Graph and SNPE Layers</h2>
<p>SNPE like many other neural network runtime engines uses layers as building blocks to define the structure of neural networks. TensorFlow on the other hand, defines a neural network as a graph of nodes and a layer is defined as a set of nodes within the graph.</p>
<p>With this in mind, in order to properly convert a TensorFlow graph into a SNPE DLC file the following requirements must be met when defining a TensorFlow graph: </p><ol>
<li>
All nodes belonging to a layer must be defined in a unique <b>TensorFlow scope</b>. </li>
<li>
A node can only belong to a single layer. </li>
</ol>
<p>More information about graph compatibility can be found in the <a class="el" href="tensorflow_graphs.html">TensorFlow Graph Compatibility</a> chapter.</p>
<h1><a class="anchor" id="tools_snpe-tensorflow-to-dlc"></a>
snpe-tensorflow-to-dlc</h1>
<p>The <b>snpe-tensorflow-to-dlc</b> tool converts a TensorFlow model into an equivalent SNPE DLC file. The options available in the tool are shown below:</p>
<pre class="fragment">$ snpe-tensorflow-to-dlc -h

usage: snpe-tensorflow-to-dlc [-h] [--input_network INPUT_NETWORK]
                              [-o OUTPUT_PATH]
                              [--copyright_file COPYRIGHT_FILE]
                              [--model_version MODEL_VERSION]
                              [--disable_batchnorm_folding]
                              [--input_type INPUT_NAME INPUT_TYPE]
                              [--input_encoding INPUT_NAME INPUT_ENCODING]
                              [--validation_target RUNTIME_TARGET PROCESSOR_TARGET]
                              [--strict] [--debug [DEBUG]] -d INPUT_NAME
                              INPUT_DIM --out_node OUT_NODE
                              [--allow_unconsumed_nodes]

Script to convert tensorflowmodel into a DLC file.

optional arguments:
  -h, --help            show this help message and exit

required arguments:
  --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.
  -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The names and dimensions of the network input layers
                        specified in the format "input_name" comma-separated-
                        dimensions, for example: "data" 1,224,224,3. Note that
                        the quotes should always be included in order to
                        handle special characters, spaces, etc. For multiple
                        inputs specify multiple --input_dim on the command
                        line like: --input_dim "data1" 1,224,224,3 --input_dim
                        "data2" 1,50,100,3.
  --out_node OUT_NODE   Name of the graph's output node.

optional arguments:
  -o OUTPUT_PATH, --output_path OUTPUT_PATH
                        Path where the converted Output model should be
                        saved.If not specified, the converter model will be
                        written to a file with same name as the input model
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of
                        the file will be added to the output model.
  --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only
                        first 64 bytes will be stored
  --disable_batchnorm_folding
                        If not specified, converter will try to fold batchnorm
                        into previous convolution layer
  --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE
                        Type of data expected by each input op/layer. Type for
                        each input is |default| if not specified. For example:
                        "data" image.Note that the quotes should always be
                        included in order to handle special characters,
                        spaces,etc. For multiple inputs specify multiple
                        --input_type on the command line. Eg: --input_type
                        "data1" image --input_type "data2" opaque These
                        options get used by DSP runtime and following
                        descriptions state how input will be handled for each
                        option. Image: input is float between 0-255 and the
                        input's mean is 0.0f and the input's max is 255.0f. We
                        will cast the float to uint8ts and pass the uint8ts to
                        the DSP. Default: pass the input as floats to the dsp
                        directly and the DSP will quantize it. Opaque: assumes
                        input is float because the consumer layer(i.e next
                        layer) requires it as float, therefore it won't be
                        quantized.Choices supported:['image', 'default',
                        'opaque']
  --input_encoding INPUT_NAME INPUT_ENCODING, -e INPUT_NAME INPUT_ENCODING
                        Image encoding of the source images. Default is bgr.
                        Eg usage: "data" rgba Note the quotes should always be
                        included in order to handle special characters,
                        spaces, etc. For multiple inputs specify
                        --input_encoding for each on the command line. Eg:
                        --input_encoding "data1" rgba --input_encoding "data2"
                        other. Use options: color encodings(bgr,rgb, nv21...)
                        if input is image; time_series: for inputs of rnn
                        models; other: if input doesn't follow above
                        categories or is unknown. Choices supported:['bgr',
                        'rgb', 'rgba', 'argb32', 'nv21', 'time_series',
                        'other']
  --validation_target RUNTIME_TARGET PROCESSOR_TARGET
                        A combination of processor and runtime target against
                        which model will be validated. Choices for
                        RUNTIME_TARGET: {cpu, gpu, dsp}. Choices for
                        PROCESSOR_TARGET: {snapdragon_801, snapdragon_820,
                        snapdragon_835}.If not specified, will validate model
                        against {snapdragon_820, snapdragon_835} across all
                        runtime targets.
  --strict              If specified, will validate in strict mode whereby
                        model will not be produced if it violates constraints
                        of the specified validation target. If not specified,
                        will validate model in permissive mode against the
                        specified validation target.
  --debug [DEBUG]       Run the converter in debug mode.
  --allow_unconsumed_nodes
                        Uses a relaxed graph node to layer mapping algorithm
                        which may not use all graph nodes during conversion
                        while retaining structural integrity.
</pre><h2><a class="anchor" id="tf_converter_usage"></a>
snpe-tensorflow-to-dlc Usage</h2>
<p>This section details the usage of the converter arguments.</p>
<p><b>input_network argument:</b> </p><ul>
<li>
The converter supports either a single frozen graph .pb file or a pair of graph meta and checkpoint files. </li>
<li>
If you are using the <a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/state_ops.html#Saver">TensorFlow Saver</a> to save your graph during training, 3 files will be generated as described below: <ol>
<li>
&lt;model-name&gt;.meta </li>
<li>
&lt;model-name&gt; </li>
<li>
checkpoint </li>
</ol>
</li>
<li>
The converter --input_network option specifies the path to the graph meta file. The converter will also use the checkpoint file to read the graph nodes parameters during conversion. The checkpoint file must have the same name without the .meta suffix. </li>
<li>
This argument is required. </li>
</ul>
<p><b>input_dim argument:</b> </p><ul>
<li>
Specifies the input dimensions of the graph's input node(s) </li>
<li>
The converter requires a node name along with dimensions as input from which it will create an input layer by using the node output tensor dimensions. When defining a graph, there is typically a placeholder name used as input during training in the graph. The placeholder tensor name is the name you must use as the argument. It is also possible to use other types of nodes as input, however the node used as input will not be used as part of a layer other than the input layer. </li>
<li>
<b>Multiple Inputs</b> <ul>
<li>
Networks with multiple inputs must provide --<b>input_dim</b> INPUT_NAME INPUT_DIM, one for each input node. </li>
</ul>
</li>
<li>
This argument is required. </li>
</ul>
<p><b>out_node argument:</b> </p><ul>
<li>
The name of the last node in your TensorFlow graph which will represent the output layer of your network. </li>
<li>
<b>Multiple Outputs</b> <ul>
<li>
Networks with multiple outputs must provide several --<b>out_node</b> arguments, one for each output node. </li>
</ul>
</li>
</ul>
<p><b>output_path argument:</b> </p><ul>
<li>
Specifies the output DLC file name. </li>
<li>
This argument is optional. If not provided the converter will create a DLC file file with the same name as the graph file name, with a .dlc file extension. </li>
</ul>
<h2><a class="anchor" id="converter_tf_example"></a>
Inception v3 Conversion Example</h2>
<pre class="fragment">$ snpe-tensorflow-to-dlc --input_network inception_v3.pb --input_dim 1,299,299,3 --output_path net.dlc --in_node input --out_node InceptionV3/Predictions/Reshape_1
--allow_unconsumed_nodes --verbose

2017-04-01 20:23:42,794 - 316 - INFO - ==============================================================
2017-04-01 20:23:42,794 - 317 - INFO - Building Network
2017-04-01 20:23:42,794 - 318 - INFO - ==============================================================
2017-04-01 20:23:42,794 - 338 - INFO - Building layer (INPUT) with nodes: Mul, shape [1, 299, 299, 3]
2017-04-01 20:23:42,794 - 348 - INFO - Converting scope (conv): [u'conv/Conv2D', u'conv/batchnorm', u'conv']
2017-04-01 20:23:43,542 - 348 - INFO - Converting scope (conv_1): [u'conv_1/Conv2D', u'conv_1/batchnorm', u'conv_1']
...
Building layer (Convolution) with nodes: [u'convolution1/Conv2D', u'convolution1/BiasAdd']
Building layer (Convolution) with nodes: [u'convolution2/Conv2D', u'convolution2/BiasAdd']
Building layer (ElementWiseSum) with nodes: [u'output/Add']
Model conversion completed!
</pre><p>The converted model can be viewed using the snpe-dlc-info tool. A subset of a sample output is shown below.</p>
<pre class="fragment">$ snpe-dlc-info -i &lt;path_to_dlc&gt;/net.dlc
DLC info for &lt;path_to_dlc&gt;/net.dlc
Model Version:
-----------------------------------------------------------------------------------------------------------------------
| Id  | Name             | Type          | Inputs  | Outputs          | Out Dims   | Parameters                       |
-----------------------------------------------------------------------------------------------------------------------
| 0   | Mul:0            | data          | Mul:0   | Mul:0            | 299x299x3  | input_preprocessing: passthrough |
| 1   | conv/Conv2D      | convolutional | Mul:0   | conv/batchnorm:0 | 149x149x32 | padding x: 0                     |
|     |                  |               |         |                  |            | padding y: 0                     |
|     |                  |               |         |                  |            | stride x: 2                      |
|     |                  |               |         |                  |            | stride y: 2                      |
|     |                  |               |         |                  |            | num filters: 32                  |
|     |                  |               |         |                  |            | kernel: 3x3                      |
|     |                  |               |         |                  |            | param count: 896 (0.00376%)      |
|     |                  |               |         |                  |            | MACs: 19M (0.336%)               |
...</pre> </div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 -->
<!-- start footer part -->
<div id="nav-path" class="navpath" font-size:small;><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
      <p align="right">
        80-NL315-14 A <br>
        MAY CONTAIN U.S. AND INTERNATIONAL EXPORT CONTROLLED INFORMATION
        <!--If the Controlled Distribution statement is to be included, uncomment below:-->
        <!--<b>Controlled Distribution - DO NOT COPY</b>-->
        <img class="footer" width:5%; alt="QTI Logo" src="images/QTI_Logo.png" />
      </p>
    </li>
  </ul>
</div>
</body>
</html>
