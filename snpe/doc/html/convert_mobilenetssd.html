<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Snapdragon Neural Processing Engine SDK: Using MobilenetSSD</title>
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" ></link>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Snapdragon Neural Processing Engine SDK
   <span id="projectnumber"></span></div>
   <div id="projectbrief">Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('convert_mobilenetssd.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Using MobilenetSSD </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="el" href="convert_mobilenetssd.html#mobilenetssd_conversion_tf">Tensorflow MobilenetSSD model</a> <br />
<a class="el" href="convert_mobilenetssd.html#mobilenetssd_conversion_caffe">Caffe MobilenetSSD model</a> <br />
 </p>
<h1><a class="anchor" id="mobilenetssd_conversion_tf"></a>
Tensorflow MobilenetSSD model</h1>
<p>Tensorflow Mobilenet SSD frozen graphs come in a couple of flavors. The standard frozen graph and a quantization aware frozen graph. The following example uses a quantization aware frozen graph to ensure accurate results on the SNPE runtimes.</p>
<p><b>Prerequisites</b></p>
<p>The quantization aware model conversion process was tested using Tensorflow v1.11 however other versions may also work. The CPU version of Tensorflow was used to avoid out of memory issues observed across various GPU cards during conversion.</p>
<p><b>Setup the Tensorflow Object Detection Framework</b></p>
<p>The quantization aware model is provided as a TFLite frozen graph. However SNPE requires a Tensorflow frozen graph (.PB). To convert the quantized model, the object detection framework is used to export to a Tensorflow frozen graph. Follow these steps to clone the object detection framework:</p>
<ul>
<li>mkdir ~/tfmodels</li>
<li>cd ~/tfmodels</li>
<li>git clone <a href="https://github.com/tensorflow/models.git">https://github.com/tensorflow/models.git</a></li>
<li>Checkout a tested object detection framework commit (SHA)<ul>
<li>git checkout ad386df597c069873ace235b931578671526ee00</li>
</ul>
</li>
</ul>
<p>Follow these installation instructions to setup the Tensorflow object detection framework:</p><ul>
<li><a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md</a></li>
</ul>
<p><b>Download the quantization aware model</b></p>
<p>A specific version of the Tensorflow MobilenetSSD model has been tested: <b>ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz</b> </p><pre class="fragment">wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz</pre><p>After downloading the model extract the contents to a directory.</p>
<pre class="fragment">tar xzvf ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz</pre><p><b>Export a trained graph from the object detection framework</b></p>
<p>Follow these instructions to export the Tensorflow graph:</p><ul>
<li><a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md</a></li>
</ul>
<p>or modify and execute this sample script</p>
<p>Create this file, export_train.sh, using your favorite editor. Modify the paths to the correct directory location of the downloaded quantization aware model files. </p><pre class="fragment">#!/bin/bash
INPUT_TYPE=image_tensor
PIPELINE_CONFIG_PATH=&lt;path_to&gt;/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/pipeline.config
TRAINED_CKPT_PREFIX=&lt;path_to&gt;/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt
EXPORT_DIR=&lt;path_to&gt;/exported
pushd ~/tfmodels/models/tfmodels/research
python object_detection/export_inference_graph.py \
--input_type=${INPUT_TYPE} \
--pipeline_config_path=${PIPELINE_CONFIG_PATH} \
--trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX} \
--output_directory=${EXPORT_DIR}
popd
</pre><p>Make the script executable</p><ul>
<li>chmod u+x export_train.sh</li>
</ul>
<p>Run the script</p><ul>
<li>./export_train.sh</li>
</ul>
<p>This should generate a frozen graph in &lt;path_to&gt;/exported/frozen_inference_graph.pb</p>
<p>Convert the frozen graph using the <a class="el" href="tools.html#tools_snpe-tensorflow-to-dlc">snpe-tensorflow-to-dlc</a> converter.</p>
<pre class="fragment">snpe-tensorflow-to-dlc --input_network &lt;path_to&gt;/exported/frozen_inference_graph.pb --input_dim Preprocessor/sub 1,300,300,3 --out_node detection_classes --out_node detection_boxes --out_node detection_scores ---output_path mobilenet_ssd.dlc --allow_unconsumed_nodes</pre><p>After SNPE conversion you should have a mobilenet_ssd.dlc that can be loaded and run in the SNPE runtimes.</p>
<p>The output layers for the model are: </p><ul>
<li>
Postprocessor/BatchMultiClassNonMaxSuppression </li>
<li>
add </li>
</ul>
<p>The output buffer names are: </p><ul>
<li>
(classes) detection_classes:0 (+1 index offset) </li>
<li>
(classes) Postprocessor/BatchMultiClassNonMaxSuppression_classes (0 index offset) </li>
<li>
(boxes) Postprocessor/BatchMultiClassNonMaxSuppression_boxes </li>
<li>
(scores) Postprocessor/BatchMultiClassNonMaxSuppression_scores </li>
</ul>
<p><br />
</p>
<h1><a class="anchor" id="mobilenetssd_conversion_caffe"></a>
Caffe MobilenetSSD model</h1>
<p>A specific version of the Caffe MobilenetSSD model has been tested: <a href="https://github.com/chuanqi305/MobileNet-SSD">Caffe MobilenetSSD</a>.</p>
<p>There is pre-trained caffe model you can download from <a href="https://github.com/chuanqi305/MobileNet-SSD">Caffe MobilenetSSD</a>.<br />
Download the following two files: </p><pre class="fragment">wget https://github.com/chuanqi305/MobileNet-SSD/blob/master/MobileNetSSD_deploy.prototxt
wget https://github.com/chuanqi305/MobileNet-SSD/blob/master/MobileNetSSD_deploy.caffemodel</pre><p><br />
Convert the model using the <a class="el" href="tools.html#tools_snpe-caffe-to-dlc">snpe-caffe-to-dlc</a> converter.</p>
<pre class="fragment">snpe-caffe-to-dlc --input_network MobileNetSSD_deploy.prototxt --caffe_bin MobileNetSSD_deploy.caffemodel --output_path caffe_mobilenet_ssd.dlc</pre><p>The input and output layers:</p>
<p>Input layer is specified in MobileNetSSD_deploy.prototxt file, via <code>input_shape</code>.<br />
By default, the output layer is the last layer as specified in MobileNetSSD_deploy.prototxt file. In this case that is <code>detection_out</code> (DetectionOutput) layer.</p>
<p><br />
To see info about converted DLC model, use <a class="el" href="tools.html#tools_snpe-dlc-info">snpe-dlc-info</a> tool</p>
<pre class="fragment">snpe-dlc-info -i caffe_mobilenet_ssd.dlc</pre><p>The PriorBox layer is folded by the converter (for model/performance optimization reasons). Consequently, PriorBox layer will not be written into DLC file, hence it will not be listed in DLC info for the model.</p>
<p><b>Training the model</b></p>
<p>To train the model in Caffe, follow instructions at <a href="https://github.com/chuanqi305/MobileNet-SSD">Caffe MobilenetSSD</a>.</p>
<p><b>Running the model in SNPE</b></p>
<p>The following are limitations and suggestions for running DLC model in SNPE: </p><ul>
<li>
Batch dimension &gt; 1 is not supported.  </li>
<li>
DetectionOutput layer is supported on CPU runtime processor only.<br />
 To run the model using different runtime processor, such as GPU or DSP, CPU fallback mode must be enabled in Runtime List (see SNPEBuilder::setRuntimeProcessorOrder() description in <a href="group__c__plus__plus__apis.html">C++ API</a>).<br />
 If using <a class="el" href="tools.html#tools_snpe-net-run">snpe-net-run</a> tool, use <code>&ndash;runtime_order</code> option  </li>
<li>
It is recommended to have all DetectionOutput layers in the network listed at the end in the .prototxt file.<br />
 This is to minimize runtime overhead incurred by CPU fallback.<br />
 <a href="https://github.com/chuanqi305/MobileNet-SSD">Caffe MobilenetSSD</a> .prototxt has DetectionOutput layer at the end by default, but if the network has more than one detection output branch, that may not be the case. <br />
 Simply edit .prototxt file, locate and move all DetectionOutput layer definitions to end of the file.  </li>
<li>
Configure DetectionOutput layer reasonably.<br />
 Performance of DetectionOutput layer (i.e. processing time) is function of layer parameters: <code>top_k</code>, <code>keep_top_k</code> and <code>confidence_threshold</code>.<br />
 For example, <code>top_k</code> parameters have practically exponential impact on processing time; e.g. top_k=100 will result in much smaller processing time vs. top_k=1000. Smaller <code>confidence_threshold</code> will result in larger number of boxes to output, and vice versa.  </li>
<li>
Resizing input dimensions at SNPE object creation/build time is not allowed.<br />
 Note that input dimensions are embedded into DLC model during conversion, but in some cases can be overridden via SNPEBuilder::setInputDimensions() (see description in <a href="group__c__plus__plus__apis.html">C++ API</a>) at SNPE object creation/build time. Due to PriorBox layer folding in the model converter, input/network resizing is not possible.  </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 -->
<!-- start footer part -->
<div id="nav-path" class="navpath" font-size:small;><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
      <p align="right">
        80-NL315-14 A <br>
        MAY CONTAIN U.S. AND INTERNATIONAL EXPORT CONTROLLED INFORMATION
        <!--If the Controlled Distribution statement is to be included, uncomment below:-->
        <!--<b>Controlled Distribution - DO NOT COPY</b>-->
        <img class="footer" width:5%; alt="QTI Logo" src="images/QTI_Logo.png" />
      </p>
    </li>
  </ul>
</div>
</body>
</html>
