<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Snapdragon Neural Processing Engine SDK: Revision History</title>
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" ></link>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Snapdragon Neural Processing Engine SDK
   <span id="projectnumber"></span></div>
   <div id="projectbrief">Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('revision_history.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Revision History </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p> <p><table class="doxtable" width="100%">  <tr>  <th colspan="1"> Version </th> <th colspan="1"> Date </th> <th colspan="1"> Description </th>  </tr>     <tr> <td> 1.43.0 </td> <td> September 2020 </td> <td> Improved the input/output conversion times for models having depth as 4 on AIP runtime.<br />
 Enabled initial support for constant layers along with elementwise Op on HTA.<br />
 Added support for opaque float concat operation in SNPE DSP concat layer.<br />
 Added support for Caffe's "Clip" layer in the caffe converter.<br />
 Added int16 example to snpe-sample app.<br />
 Fixed the crash while running multi-threading applications with user buffer mode on AIP runtime.<br />
 Fixed bug in ONNX converter that used a hard-coded name for the sequence length input of the LSTM operator.<br />
 Fixed bug in ONNX converter for Unsqueeze layer, which got a key-error with static inputs.<br />
 Fixed the bug in l2_fetch usage during output conversion which improved the performance significantly for some models running on AIP runtime.<br />
 Fixed the issue with generation of HTA enabled dlc for denoise model.<br />
 Fixed the segmentation fault issue during dlc generation with specific inputs on HTA.<br />
 Fixed issue with PlatformValidator.hpp reference to non-existent #include.<br />
  </td> </tr>  <tr> <td> 1.42.2 </td> <td> September 2020 </td> <td> Fixed the bug in l2_fetch usage during output conversion which improved the performance significantly for some models running on AIP runtime.<br />
  </td> </tr>  <tr> <td> 1.42.0 </td> <td> August 2020 </td> <td> Removed V60 DSP libs from SNPE SDK.<br />
 Enabled the AIP runtime support for generating the intermediate outputs from HTA with online compiler.<br />
 Enabled multithread for re-quantize process in DSP runtime.<br />
 Added optional parameter to set the hysteris period for sustained high and burst profiles in DSP runtime.<br />
 Added support for opaque float concat operation in SNPE DSP concat layer.<br />
 Fixed bug in UserBufferTF8 where retrieving the encoding would always return null.<br />
 Fixed box decoder performance issue on mobilenet v2 ssd model for DSP runtime.<br />
 Fixed tanh performance issue by replacing QuantizedTanh_8_ref with QuantizedTanh_8 op in DSP runtime.<br />
  </td> </tr>  <tr> <td> 1.41.0 </td> <td> July 2020 </td> <td> Added MatMul support on the CPU runtime.<br />
 Added support for new version of 7250 with integrated PMIC module.<br />
 User Defined Operations(UDO) with weight parameters have been added to demonstrate both quantization and network execution on CPU and DSP runtime cores respectively.<br />
 Optimized tile Op in DSP runtime, that used 2d memcpy for w-d plane tiling and HVX for tiling along depth.<br />
 Fixed stack overflow issue in concat layer in DSP runtime.<br />
 Fixed issue with input for multibatch in DSP runtime.<br />
 Fixed issue in TF converter that prevented FusedBatchNorm operations from being merged into previous Convolution layer.<br />
 Fixed DSP crash issue due to stack overflow Concat layer preparation.<br />
  </td> </tr>  <tr> <td> 1.40.0 </td> <td> June 2020 </td> <td> Added DSP Graph Caching support for AIP models with HVX subnets.<br />
 Upgraded DSP to use Hexagon SDK 3.5.2 toolchain.<br />
 Added support for 16 bit UDO layers in DSP.<br />
 Added support for large average pooling, reduce_mean layer and improved elemetnwise_mul support for larger tensor size.<br />
 Fixed the issue with buffer ordering during the execution of batched models on AIP runtime.<br />
 Fixed issue with SsdDetectionOut when number of classes is only 1.<br />
 Fixed accuracy issue with Correlation 1D op.<br />
 Fixed improper processing when 16bit input quantization is used in certain cases.<br />
 Fixed scaling logic in convert_16 op.<br />
  </td> </tr>  <tr> <td> 1.39.1 </td> <td> May 2020 </td> <td> Fixed the performance regression of Mobilenet SSD model on AIP runtime.<br />
  </td> </tr>  <tr> <td> 1.39.0 </td> <td> May 2020 </td> <td> The SNPE license (LICENSE.pdf) has been updated, please review it for more details. Additionally the REDIST.txt has been removed, as redistribution is covered in the license.<br />
 Added graph caching support which improves init times for DSP &amp; AIP networks. (DSP subnet with in AIP is not supported)<br />
 Optimized Prelu to reduce saturation loss during re-quantization at prelu by using cubic approximation in AIP runtime.<br />
 Fixed the input conversions to allocate the required buffers during initialization itself, to improve the inference time for AIP runtime.<br />
 Fixed potential bug with freeing threads in DSP runtime.<br />
 Added additional logging messages for debugging in DSP runtime.<br />
 Added support for the AIP runtime in the SNPE sample "snpe-sample".<br />
 Added support for BBox transform layer in Caffe2 converter.<br />
 Added new opset support in the ONNX converter: ArgMax, ArgMin, Concat, PRelu, ReduceMean, ReduceMax, ReduceMin, ReduceSum, Squeeze, Unsqueeze, MatMul, Flatten, Max, Split, Clip.<br />
 Added support for the fixed-point version of the MobileNetV3 model with H-Swish neuron in TF converter.<br />
 Improved support of resizing in Crop layer for TF and Caffe converter by introducing new “counts” parameter.<br />
 Fixed issue of incorrect UDO tensor datatype in quantizer.<br />
 Fixed issue with setting the performance profile mode for HTA from AIP runtime in multi-threading use cases that could cause performance to drop.<br />
 Fixed issue with snpe_bench.py memory profiling.<br />
  </td> </tr>  <tr> <td> 1.38.0 </td> <td> April 2020 </td> <td> Enabled FC/MatMul to use VTCM if available in DSP.<br />
 Optimized 16-bit MeanVarianceNormalize in DSP runtime.<br />
 Added support batchwise scalar divide operation in DSP runtime.<br />
 Optimized Hard-swish operator for mobilenetV3.<br />
 Added support for EltwiseMin layer for ONNX converter and CPU runtime.<br />
 Added support for Onnx BatchNorm layer (OpVer 9, 12) in Onnx Converters.<br />
 Caffe preprocessing subtract_mean layer is added. If specified, converter will enable preprocessing specified by a data layer transform_param subtract_mean.<br />
 ONNX softmax converter support only existed for rank &lt;= 2. Support for tensors rank &lt;= 4 was added.<br />
 Enabled the end-user / developer to request the use of an unsigned process domain to avoid the requirement of signed libraries for SNPE execution on 8250 and newer devices.<br />
 Removed autoquantization for classes output in MultiClassNMS layer and added support for float addition in ElementwiseOp layer to handle this case.<br />
 Fixed the issue with enabling stats for AIP runtime on models where number of layers in HTA subnet is more than SNPE layers.<br />
 Fixed the output conversions to allocate the required buffers during initialization itself in AIP runtime, to improve the inference time.<br />
 Enabled honoring of padding information from the HTA driver which is pre-computed by AIP runtime earlier, to unblock execution of more models.<br />
 Fixed the issue with output buffer id while converting depth2space to deconv on HTA.<br />
 Fixed a bug during graph transformation while folding the batchnorm on HTA.<br />
 Increased DCVS relaxed sleep latency duration, this will let power system know that CDSP can goto deeper sleep state. If there is no active request for inferencing, it is better for system to go in deeper sleep state.<br />
  </td> </tr>  <tr> <td> 1.37.0 </td> <td> March 2020 </td> <td> Enabled the online compiler support for HTA 1.x family of devices.<br />
 AIP performance profiles behavior is aligned similar to DSP runtime for reduced power consumption in case of inference inactivity.<br />
 ONNX Converter: Added support for Onnx Pad layer (OpVer 11).<br />
 Added support for the h-swish layer used by MobileNet V3.<br />
 Removed support for the Generate Proposals, ROI Align, and ROI Proposal layers.<br />
 Added improved support for the reporting of Exceptions in the Java API.<br />
 Updated the DSP UDO header file to be compatible with SNPE 1.37.0.<br />
 The DSP UDO support is updated to be compatible with Hexagon SDK 3.5.1.<br />
 The network creation action was moved onto another thread to avoid impacting the affinity for the main thread of the calling program.<br />
 Snpe-dlc-info: Fixed issue in MACs calculation error for deconvolution layer.<br />
 Avoid crash on SDM845 and other v65 targets when unable to retrieve VTCM memory.<br />
 Fixed an issue in the TensorFlow converter where the weights in the Fully Connected layer were incorrectly transposed.<br />
 Fixed the support for using DSP UDO with the AIP runtime. Previously, the UDO packages would not be properly loaded in the AIP runtime.<br />
 Fixed DiagLog data for a UDO on GPU, where it did not report proper values for start and stop.<br />
 Enable support for keras batchnorm with empty mean and variance to a default values.<br />
 Fixed a memory leak when using IsRuntimeAvailable() with the VOLATILE_CHECK for the DSP runtime.<br />
  </td> </tr>  <tr> <td> 1.36.0 </td> <td> February 2020 </td> <td> Added Java API extension to register UDO package with SNPE.<br />
 snpe-dlc-info now prints the command-line that was used to quantize the DLC if applicable.<br />
 Added support to handle UDO layers with multiple TF8 outputs with different quantization parameters.<br />
 Added support for an additional profiling level (moderate) for SNPE benchmarking script and associated snpe-net-run executable for tracking initialization time metrics.<br />
 Upgraded DSP to use Hexagon SDK 3.5.1 toolchain.<br />
 Extend Platform Validator to detect HTA API version.<br />
 Add VOLATILE_CHECK Mode for SNPE DSP Runtime Checking to query runtime availability in each call instead of giving cached result.<br />
 Performance modes like LOW_POWER_SAVER, HIGH_POWER_SAVER, LOW_BALANCED added for CPU runtime.<br />
 Fixed bug with propagation of model version during conversion.<br />
 Fixed the issue with selecting the correct output shape during graph transformation while inserting1x1 conv2d for different input format.<br />
 Fixed the issue with allocation of layer descriptor while loading the network on HTA.<br />
  </td> </tr>  <tr> <td> 1.35.0 </td> <td> January 2020 </td> <td> Introduce the User-Defined Operations (UDO) feature.<br />
 Added support for SDM720G/SM7125.<br />
 Added support to snpe-throughput-net-run for UserBuffer input tensors (both INT8 and INT16).<br />
 Input batching support is added for networks that can run completely on AIP runtime.<br />
 Add support for the tf.stack and tf.unstack ops to the DSP and CPU runtimes.<br />
 Add support for the tf.stack, tf.unstack, tf.floor, tf.minimum to the TF converter.<br />
 Fixed some small memory leaks that are seen when repeatedly calling dlopen()/dlclose() on libSNPE.so.<br />
 Updated the Deconvolution operation on DSP with a new kernel that improves performance on various kernel sizes and strides.<br />
 Fix ssd_detection CDSP crash on DSP runtime.<br />
 Updated the HTA to partition the input layer, if it has a connection to a layer that is not included in the same partition.<br />
 Improved the tiling configuration support for depth wise convolution layer.<br />
  </td> </tr>  <tr> <td> 1.34.0 </td> <td> January 2020 </td> <td> Initial support for ops with 16-bit activations using HTA in both snpe-dlc-quantize and in the SNPE AIP runtime.<br />
 New option for snpe-net-run to automatically turn unconsumed tensors of the network (tensors that are not inputs to a layer) into network outputs.<br />
 Fixed inconsistent results on SM8250 in certain cases for depthwise convolutions.<br />
 Add support for the depth2space operation on the GPU.<br />
 Using optimized Softmax implementation in AIP networks when input activation has more than 5000 elements.<br />
 Truncate detection output on DSP to return valid data only.<br />
 Ensure weights are properly flushed to DDR for use during inference in the DSP runtime.<br />
 Fix support for NV21 encoding in the DSP runtime.<br />
  </td> </tr>  <tr> <td> 1.33.2 </td> <td> November 2019 </td> <td> Address accuracy issues for Deconvolution in the AIP runtime.<br />
 Changed behavior of Crop layer resize, so it retains the number of copied elements on each dimension.<br />
 Make quantizer &ndash;override_params work for AIP.<br />
 Reordered PerformanceProfile_t to be ABI compatible with 1.32.0.<br />
 Using optimized Softmax implementation in AIP networks when input activation has more than 5000 elements.<br />
  </td> </tr>  <tr> <td> 1.33.1 </td> <td> November 2019 </td> <td> Fixed a build issue that incorrectly removed Symphony.<br />
  </td> </tr>  <tr> <td> 1.33.0 </td> <td> November 2019 </td> <td> New performance modes have been added:<br />
 </p><ul>
<li>
LOW_POWER_SAVER: Run in lower clock than POWER_SAVER, at the expense of performance.<br />
 </li>
<li>
HIGH_POWER_SAVER: Run in higher clock and provides better performance than POWER_SAVER.<br />
 </li>
<li>
LOW_BALANCED: Run in lower balanced mode, provides lower performance than BALANCED.<br />
 </li>
</ul>
<p>snpe-dlc-info adds a summary of the layer types in use in the model.<br />
 Updated to use new BLAS functionality that leverages OpenMP. This adds a new dependency on the OpenMP shared library for Linux platforms.<br />
 Added 32-bit bias support.<br />
 Support init caching for SSD output layer on DSP.<br />
 Fix memory leak causing increasing init time on DSP.<br />
 Add converter support for dilated convolution when used with fakequant nodes.<br />
 Multiple bugs fixed in snpe-onnx-to-dlc that were causing errors for models having torch.Mul op.<br />
 Extends TF converter support to NMSv1 Op in addition to existing support for v2 and v3 NMS Ops.<br />
 Tensorflow conversion bug fixed in infer_shape for StridedSlice Op. output_shape should not be a list of shapes but the shape of the one output.<br />
 Fix bug with propagation of model version during conversion.<br />
 If burst mode is set, set thread affinity to Big Cores during init and de-init, and restore to the previous setting after the actions are complete.<br />
 Fix segfault when using user buffers with a resizable dimension.<br />
  </td> </tr>  <tr> <td> 1.32.0 </td> <td> Oct 2019 </td> <td> Add Caffe MVN Layer support in the Caffe Converter, CPU Runtime, and DSP Runtime<br />
 snpe-dlc-quantize: Enable the use of quantization parameters calculated during training when using dlc quantizer. To override the SNPE generated quantization parameters pass &ndash;override_params to snpe-dlc-quantize.<br />
 Removed deprecated command line arguments from converters. All three converters now require passing -i/&ndash;input_network for model input paths.<br />
 snpe-dlc-diff: Added command-line option [&ndash;diff_by_id/-i] to snpe-dlc-diff. This option allows users to compare 2 models in order(sorted by id)<br />
 Added support for L2Norm layer to TensorFlow converter<br />
 Optimized the DSP performance for the 'Space To Depth' layer<br />
 Add support in the Java API for setInitCacheEnabled(), and setStorageDirectory() to enable DLC caching support.<br />
 Allow graceful recovery after a fastrpc error - Recreate the userPD after the cDSP crashes so that the user can continue on the SNPE process with subsequent instances, instead of having to close the SNPE process. Note: all the instance associated to the previous userPD will be lost.<br />
 snpe-dlc-viewer: Associate each layer type to a fixed color for consistency when using snpe-dlc-viewer<br />
 Split the SNPE isRuntimeAvailable method into two separate functions to improve backward compatibility with existing client binaries that were built against the older signature.<br />
 TF Converter: Fix Elementwise Broadcast support<br />
 ONNX Converter: Fixed bug where output dimension was incorrect when keep_dims parameter was set to False for Argmax, ReduceSum and ReduceMax.<br />
 ONNX Converter: Fixed bug where pad attribute was not properly parsed for Deconv Op.<br />
 Caffe Converter: Fixed bug when converting SSD-based models when using Python 3.<br />
 TF Converter: Fixed bug where converter was removing const Op input to reshape op when passed through identity op(s). i.e const-&gt; identity -&gt; reshape.<br />
 Fixed bug where getOutputSize() would give the wrong result on output tensors in UserBuffer mode<br />
  </td> </tr>  <tr> <td> 1.31.0 </td> <td> September 2019 </td> <td> New patterns were added to enable running the CLE algorithm on more op patterns and model architectures.<br />
 Added Tensorflow converter support for Caffe-style SSD networks.<br />
 Added support for HeatmapMaxKeypoint layer in the CPU runtime.<br />
 Added support for ROI Align layer in CPU runtime.<br />
 Added initial L2Norm layer support in CPU runtime. No support for axis parameter yet: normalization is performed along the inner-most dimension of the input tensor.<br />
 Support for single-input Concatenation layers was added to CPU, GPU and DSP.<br />
 Changed determination of number of batch dimensions in the Fully Connected layer so rank greater than 1 is always assumed to mean that there is 1 batch dimension.<br />
 Removed constraint on the LSTM layer in the GPU runtime that prevented batch mode operation.<br />
 Added support for Leaky-RELU in the TensorFlow converter. Both the actual Leaky-Relu op and the elementwise op representation are supported and map to SNPE's Prelu op.<br />
 Added Argmax support to the Caffe converter, and optimized performance on the DSP runtime.<br />
 Added new column to snpe-dlc-info that displays the supported runtimes for each layer.<br />
 Fixed an edge case where in certain conditions OpenCL would return CL_INVALID_WORK_GROUP_SIZE.<br />
 Made isRuntimeAvailable Java API thread-safe.<br />
 Replace unstable image from sample Android classifier application data set with an image that is more consistent.  </td> </tr>  <tr> <td> 1.30.0 </td> <td> August 2019 </td> <td> Documentation has been added to reflect the new common converter command line options for input processing; Converters now propagate required batchnorm information for performing quantization optimizations; Support for the new bias correction quantization optimization which adjusts biases by analyzing float vs quantized activation errors and adjusting the model to compensate; ONNX converter now filters single input Concats as a no ops as SNPE didn’t support them; Converter input processing now uniformly handles different input types and encodings; ONNX converter now supports the ConvTranspose ‘output_padding’ attribute by adding an additional pad layer after the ConvTranspose op; Integrates the latest flatbuffer 1.11 library which brings speed improvements and options for model size reduction; GPU size limitations with the ArgMax op (when setting the keepDims op attribute to false) can be worked around by enabling CPU fallback; Fixed DSP error with MobileNet SSD on QCS403 and QCS405; Fixed the issue with partitioning of deconv layer in HTA; </td> </tr>  <tr> <td> 1.29.0 </td> <td> July 2019 </td> <td> Added support for dlc reorder tool;Optimization of HTA d32 conversions;Added tf space_to_depth op for SNPE CPU and DSP runtime;Benchmarking scripts enhanced for showing further break down of execution time, across various components;Added support for additional ONNX binary element-wise ops;Optimized deconv layer for improving performance;Fixed an issue related to runtime error in DSP runtime;Performance Optimization of SNPE GPU Runtime for Shufflenet V2 by using profiling level config </td> </tr>  <tr> <td> 1.28.0 </td> <td> June 2019 </td> <td> Added an optional argument to isRuntimeAvailable for the DSP runtime so that it doesn't activate the DSP; Allow UB_T8 and UB_FLOAT output for snpe-net-run; Added a new command line option for snpe-dlc-diff to check layer names; Updated the &ndash;dlc argument to &ndash;output_path for snpe-caffe-to-dlc to align with the ONNX converter; Added &ndash;dry_run argument to snpe-onnx-to-dlc to allow evaluation for successful conversion on an ONNX model; Added support for the gather op in the DSP runtime; Added support to convert the TF MobileNet-V1-FPN-SSD model; Fixed a memory leak in the DSP runtime that is seen when repeatedly loading and unloading a network; Addressed issues on V66 DSPs related to acquiring VTCM memory; Fixed an issue related to multiple inputs for the Caffe converter; Fixed an issue in the TF converter related to element-wise sun and the atrous parameter; Fixed an issue in the TF converter related to tf.crop_and_resize when there are only 2 inputs.; Fixed additional cases of uncaught exceptions with the aarch64-android-clang6.0 platform; </td> </tr>  <tr> <td> 1.27.0 </td> <td> May 2019 </td> <td> Added new APIs support for setting output tensor names to snpeBuilder and to fetch output tensor names for a given output layer name; Improved the peak memory usage with DLC v3 format; Fixed few issues with performance and runtime failures on DSP runtime; Fixed few issues and improved error handling for platform validator; Fixed the issues with Pooling and Instance norm layers of Tensorflow converter; Removed *-android-gcc4.9 platform support. This compiler has been retired for the Android NDK, so all support is transitioning to using Clang for Android; Removed arm-linux-gcc4.8hf platform. The development platform has been retired; </td> </tr>  <tr> <td> 1.26.0 </td> <td> Apr 2019 </td> <td> Added support for the ONNX Gather Op in the ONNX Converter and CPU runtime; Optimized DeConvolution Layer for the DSP runtime; Support for tf.nn.moments in the TF converter, CPU and DSP runtimes; Added TF Reflect Pad support for the DSP runtime; Add symmetric quantizer option in snpe-dlc-quantize; Add support for batch &gt; 1 when using the Scale Layer on the DSP runtime; Updated Platform Validator python script to be OS-independent; Added additional optimizations for HTA input conversion; </td> </tr>  <tr> <td> 1.25.0 </td> <td> Mar 2019 </td> <td> Updated DLC format to improve load time performance and memory consumption. Old DLCs will continue to work as is, but new DLCs generated from 1.25 will use the new format; Added support for optimized; MultiClassNms and ArgMax ops on DSP runtime; Added option to request larger memory allocations on the DSP for improved init time, at the expense of more memory use; Improved concurrency for multiple; SNPE objects running simultaneously on DSP; Improvements when using priority control on DSP; Added support for channel shuffle and ArgMax in the ONNX converter; Support multiple subnets within the AIP runtime; </td> </tr>  <tr> <td> 1.24.0 </td> <td> Feb 2019 </td> <td> Adding setProfilingLevel API support for AIP and CPU runtimes; Various stability issues on aip runtimes are addressed;Added support for Snapdragon 712;Support multi inputs and multiple outputs on each SNPE AIP’s subnet </td> </tr>  <tr> <td> 1.23.0 </td> <td> Jan 2019 </td> <td> Upgrade to Android NDK r17c to build SNPE; Improving initialization and de-initialization times; Various DSP timing fixes; Addressed some DSP concurrency edge cases that could impact output values; TF converter support for non max suppression, crop and resize Ops </td> </tr>  <tr> <td> 1.22.0 </td> <td> Nov 2018 </td> <td> Support for several new ops on DSP runtime; Upgrade to Android NDK r16b to build SNPE; setProfilingLevel API support in DSP runtime; Added new tool snpe-throughput-net-run </td> </tr>  <tr> <td> 1.21.0 </td> <td> Oct 2018 </td> <td> Tensorflow converter and CPU runtime support for various ops; DSP runtime support for Eltwise Realdiv and Square ops; GPU support for resize_align_corners layer </td> </tr>  <tr> <td> 1.20.0 </td> <td> Sep 2018 </td> <td> Support for QCS605 LE platform; NDK version upgrade to r14b; Tensorflow converter support for elementwise sqrt and softmax with dimension &gt; 2; Platform validation command line tool </td> </tr>  <tr> <td> 1.19.0 </td> <td> Aug 2018 </td> <td> ELU op support for Tensorflow/Onnx Converters and CPU/GPU runtimes; BoxWithNMSLimit and BBoxTransform ops support in caffe2 converter; Support for Caffe Power Layer in GPU </td> </tr>  <tr> <td> 1.18.0 </td> <td> Jul 2018 </td> <td> Support for pad and elementwise subtraction on GPU; ONNX converter support for shape and pad ops; Tensorflow converter support for additional ops </td> </tr>  <tr> <td> 1.17.0 </td> <td> Jun 2018 </td> <td> Support for Scale Layer in Caffe converter and DSP runtime, DSP support for batch&gt;1 and ChannelShuffle, Updated SDK examples for Inception v3 2016 model </td> </tr>  <tr> <td> 1.16.2 </td> <td> May 2018 </td> <td> Remove linkage to libstdc++.so in DSP loader libraries </td> </tr>  <tr> <td> 1.16.1 </td> <td> May 2018 </td> <td> Remove linkage to libstdc++.so, DSP runtime fixes, fix for 1D BatchNorm </td> </tr>  <tr> <td> 1.16.0 </td> <td> May 2018 </td> <td> Batch&gt;1 support (except DSP runtime); layer optimizations for DSP runtime; Caffe2 ChannelShuffle support (except DSP runtime) </td> </tr>  <tr> <td> 1.15.2 </td> <td> Mar 2018 </td> <td> Fix for GPU runtime memory leak and reshape to/from 1D </td> </tr>  <tr> <td> 1.15.1 </td> <td> Apr 2018 </td> <td> Fix for converter for instance normalization followed by scale </td> </tr>  <tr> <td> 1.15.0 </td> <td> Apr 2018 </td> <td> Support for instance normalization for Caffe and Caffe2, MobilenetSSD (Caffe) </td> </tr>  <tr> <td> 1.14.1 </td> <td> Mar 2018 </td> <td> Minor fixes </td> </tr>  <tr> <td> 1.14.0 </td> <td> Mar 2018 </td> <td> ONNX converter (alpha), multiple enhancements and fixes </td> </tr>  <tr> <td> 1.13.0 </td> <td> Feb 2018 </td> <td> GPU and DSP v65 performance improvements. GPU floating point 16 support. </td> </tr>  <tr> <td> 1.12.0 </td> <td> Jan 2018 </td> <td> Support for Android LLVM/libc++, MobilenetSSD (TensorFlow) </td> </tr>  <tr> <td> 1.10.1 </td> <td> Dec 2017 </td> <td> Fix a bug in the DSP runtime when using mixed userbuffer input types </td> </tr>  <tr> <td> 1.10.0 </td> <td> Dec 2017 </td> <td> Support for Mobilenet on DSP, enhanced DSP runtime, Snapdragon Flight Board, updates for UserBuffers </td> </tr>  <tr> <td> 1.8.0 </td> <td> Nov 2017 </td> <td> Mobilenet support on CPU, GPU, Support for Snapdragon 636 and Android 64 bit </td> </tr>  <tr> <td> 1.6.0 </td> <td> Oct 2017 </td> <td> Support for Snapdragon 450, minor updates and fixes </td> </tr>  <tr> <td> 1.4.0 </td> <td> Aug 2017 </td> <td> Support for Snapdragon 630, FasterRCNN and ADSP on AGL </td> </tr>  <tr> <td> 1.2.2 </td> <td> July 2017 </td> <td> QDN release </td> </tr>  <tr> <td> 1.2.0 </td> <td> June 2017 </td> <td> Beta Caffe2 Converter </td> </tr>  <tr> <td> 1.0.2 </td> <td> May 2017 </td> <td> Support for 820AGL platform, Snapdragon 660, and Compute DSP on Android </td> </tr>  <tr> <td> 1.0.1 </td> <td> Apr 2017 </td> <td> Documentation update only </td> </tr>  <tr> <td> 1.0 </td> <td> Apr 2017 </td> <td> </td> </tr>  </table></p>  </div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 -->
<!-- start footer part -->
<div id="nav-path" class="navpath" font-size:small;><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
      <p align="right">
        80-NL315-14 A <br>
        MAY CONTAIN U.S. AND INTERNATIONAL EXPORT CONTROLLED INFORMATION
        <!--If the Controlled Distribution statement is to be included, uncomment below:-->
        <!--<b>Controlled Distribution - DO NOT COPY</b>-->
        <img class="footer" width:5%; alt="QTI Logo" src="images/QTI_Logo.png" />
      </p>
    </li>
  </ul>
</div>
</body>
</html>
