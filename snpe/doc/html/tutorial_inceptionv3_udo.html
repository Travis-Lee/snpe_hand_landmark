<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Snapdragon Neural Processing Engine SDK: UDO Tutorial</title>
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" ></link>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Snapdragon Neural Processing Engine SDK
   <span id="projectnumber"></span></div>
   <div id="projectbrief">Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('tutorial_inceptionv3_udo.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">UDO Tutorial </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="inceptionv3_udo_overview"></a>
Overview</h1>
<p>This tutorial describes the steps needed to create a UDO package and execute the Inception-V3 model using the package. The Softmax operation has been chosen in this tutorial to demonstrate the implementation of a UDO with SNPE.</p>
<p>The SNPE SDK provides the resources for this example under</p><ul>
<li>$SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax</li>
</ul>
<p>Information on UDO in general is available at <a class="el" href="udo_overview.html">UDO Overview</a>.</p>
<p>Information on running the Inception-V3 network without UDO is available at <a class="el" href="tutorial_inceptionv3.html">Inception-V3 Tutorial</a>.</p>
<h1><a class="anchor" id="inceptionv3_udo_prerequisites"></a>
Prerequisites</h1>
<p>The following tutorial assumes that general <a class="el" href="setup.html#installation">SNPE setup</a> has been followed to support SDK environment, TensorFlow environment, and desired platform dependencies. The steps listed in this tutorial use the Tensorflow model in the form of inception_v3_2016_08_28_frozen.pb. For details on acquiring the Inception-V3 model visit <a class="el" href="tutorial_setup.html#tutorial_setup_inception_v3">Tutorials Setup</a>.</p>
<h1><a class="anchor" id="inceptionv3_udo_introduction"></a>
Introduction</h1>
<p>Here are the steps to develop and run a UDO</p>
<p>1.) <a class="el" href="tutorial_inceptionv3_udo.html#inceptionv3_udo_package">Package Generation</a></p>
<p>2.) <a class="el" href="tutorial_inceptionv3_udo.html#inceptionv3_udo_conversion">Framework Model Conversion to a DLC</a></p>
<p>3.) <a class="el" href="tutorial_inceptionv3_udo.html#inceptionv3_udo_impl">Package Implementation</a></p>
<p>4.) <a class="el" href="tutorial_inceptionv3_udo.html#inceptionv3_udo_compilation">Package Compilation</a></p>
<p>5.) <a class="el" href="tutorial_inceptionv3_udo.html#inceptionv3_udo_execution">Model Execution</a></p>
<p>Steps 1-4 are run offline on the x86 host and are necessary for execution in step 5. Step 5 provides information on execution using the SNPE command-line executable snpe-net-run. Optionally, the user can perform steps 1-4 automatically using the provided <a class="el" href="tutorial_inceptionv3_udo.html#inceptionv3_udo_setup">setup script</a>.</p>
<h1><a class="anchor" id="inceptionv3_udo_package"></a>
Step 1: Package Generation</h1>
<p>Generating the SoftmaxUdo package requires the <b>snpe-udo-package-generator</b> tool and the provided UDO plugin: Softmax.json. The plugin is located under $SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/config. More information about creating a UDO plugin can be found <a class="el" href="udo_operator_definition.html#udo_config_spec">here</a>.</p>
<p>Generate the SoftmaxUdo Package using the following: </p><pre class="fragment">export SNPE_UDO_ROOT=$SNPE_ROOT/share/SnpeUdo
snpe-udo-package-generator -p $SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/config/Softmax.json -o $SNPE_ROOT/models/inception_v3/
</pre><p>This command creates the Softmax based package at $SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage.</p>
<p>For more information on the snpe-udo-package-generator tool visit <a class="el" href="creating_udo_package.html">here</a>.</p>
<h1><a class="anchor" id="inceptionv3_udo_conversion"></a>
Step 2: Framework model Conversion to a DLC</h1>
<p>Converting the Tensorflow Inception-V3 model to DLC requires the <b>snpe-tensorflow-to-dlc</b> tool. The snpe-tensorflow-to-dlc tool consumes the same Softmax.json used in package generation via the --udo command line option. In this step, &lt;INCEPTION_V3_PATH&gt; refers to the path to the inception_v3 pb file. For example, after running the setup_inceptionv3.py script &lt;INCEPTION_V3_PATH&gt; is $SNPE_ROOT/models/inception_v3/tensorflow.</p>
<p>Convert Inception-V3 with the following: </p><pre class="fragment">snpe-tensorflow-to-dlc --input_network &lt;INCEPTION_V3_PATH&gt;/inception_v3_2016_08_28_frozen.pb --input_dim 'input' 1,299,299,3 --out_node InceptionV3/Predictions/Reshape_1 --output_path $SNPE_ROOT/models/inception_v3/dlc/inception_v3_udo.dlc --allow_unconsumed_nodes --udo $SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/config/Softmax.json
</pre><p>This will generate a DLC named inception_v3_udo.dlc containing the Softmax as UDO at $SNPE_ROOT/models/inception_v3/dlc.</p>
<h1><a class="anchor" id="inceptionv3_udo_impl"></a>
Step 3: Package Implementations</h1>
<p>The generated package creates the skeleton of the operation implementation, which must be filled by the user to create a functional UDO. Additionally, the skeleton for a user implemented validation function can be populated to validate information about the UDO passed from the SNPE runtime. The rest of the code scaffolding for compatibility with SNPE is provided by the snpe-udo-package-generator.</p>
<p>The UDO implementations and validation function for this tutorial are provided under $SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/src.</p>
<p><b>Note:</b> Implementing validation is user-specific and not a strict requirement for using a UDO in SNPE. Only CPU Validation is provided as example.</p>
<p><b>CPU Implementations (Android and x86)</b></p>
<p>The files in the package that need to be implemented for CPU are</p><ul>
<li>SoftmaxUdoPackage/jni/src/CPU/SoftmaxImplLibCpu.cpp</li>
<li>SoftmaxUdoPackage/jni/src/reg/SoftmaxUdoPackageCpuImplValidationFunctions.cpp</li>
</ul>
<p>The provided example implementations for these files are at the locations</p><ul>
<li>$SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/src/CPU/SoftmaxImplLibCpu.cpp</li>
<li>$SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/src/reg/SoftmaxUdoPackageCpuImplValidationFunctions.cpp</li>
</ul>
<p>Copy the provided implementations to the package: </p><pre class="fragment">cp -f $SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/src/CPU/SoftmaxImplLibCpu.cpp $SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage/jni/src/CPU/
cp -f $SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/src/reg/SoftmaxUdoPackageCpuImplValidationFunctions.cpp $SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage/jni/src/reg/
</pre><p>Optionally, the user can provide their own implementations in the package.</p>
<p><b>GPU Implementations</b></p>
<p>The file in the package that needs to be implemented for GPU is</p><ul>
<li>SoftmaxUdoPackage/jni/src/GPU/SoftmaxImplLibGpu.cpp</li>
</ul>
<p>The provided example implementation is at the location</p><ul>
<li>$SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/src/GPU/SoftmaxImplLibGpu.cpp</li>
</ul>
<p>Copy the provided implementation to the package: </p><pre class="fragment">cp -f $SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/src/GPU/SoftmaxImplLibGpu.cpp $SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage/jni/src/GPU/
</pre><p>Optionally, the user can provide their own implementations in the package.</p>
<p><b>DSP Implementations</b></p>
<p>Similar to all other SNPE runtimes, a registration library and an implementation library are required to run inference on a network with UDO layers on SNPE DSP. The registration library will run on CPU, and specifies the DSP implementation library of the UDO.</p>
<p>Please note that only C files are supported for UDO on DSP runtime.</p>
<p>The file in the package that need to be implemented for DSP are</p><ul>
<li>SoftmaxUdoPackage/jni/src/DSP/SoftmaxImplLibDsp.c</li>
<li>SoftmaxUdoPackage/include/SoftmaxImplLibDsp.h</li>
</ul>
<p>The provided example implementation is at the location</p><ul>
<li>$SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/src/DSP/SoftmaxFloatImpl/SoftmaxImplLibDsp.c</li>
<li>$SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/src/DSP/SoftmaxFloatImpl/SoftmaxImplLibDsp.h</li>
</ul>
<p>Copy the provided implementations to the package: </p><pre class="fragment">cp -f $SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/src/DSP/SoftmaxFloatImpl/SoftmaxImplLibDsp.c $SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage/jni/src/DSP/
cp -f $SNPE_ROOT/examples/NativeCpp/UdoExample/Softmax/src/DSP/SoftmaxFloatImpl/SoftmaxImplLibDsp.h $SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage/include/
</pre><p>Optionally, the user can provide their own implementations in the package.</p>
<h1><a class="anchor" id="inceptionv3_udo_compilation"></a>
Step 4: Package Compilation</h1>
<p><b>x86 Host Compilation</b></p>
<p>Compiling on x86 host uses the make build system. Compile the CPU implementations with the following: </p><pre class="fragment">cd $SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage
make cpu_x86
</pre><p>The exepcted artifacts after compiling for CPU on x86 host are</p><ul>
<li>SoftmaxUdoPackage/libs/x86-64_linux_clang/libUdoSoftmaxUdoPackageImplCpu.so</li>
<li>SoftmaxUdoPackage/libs/x86-64_linux_clang/libUdoSoftmaxUdoPackageReg.so</li>
</ul>
<p><b>Android CPU Runtime Compilation</b></p>
<p>Compilation for the CPU runtime on Android uses Android NDK. The ANDROID_NDK_ROOT environment variable must be set to the directory containing ndk-build in order to compile the package. </p><pre class="fragment">export ANDROID_NDK_ROOT=&lt;path_to_android_ndk&gt;
</pre><p>It is suggested to add ANDROID_NDK_ROOT to the PATH environment variable to access ndk-build. </p><pre class="fragment">export PATH=$ANDROID_NDK_ROOT:$PATH
</pre><p>Target architecture must also be specified when compiling the package. </p><pre class="fragment">export UDO_APP_ABI=&lt;target_architecture&gt;
</pre><p>This tutorial uses arm64-v8a architectures - it is recommended but not required to use arm64-v8a as the target architecture for the remainder of the tutorial. If no target architecture is supplied both arm64-v8a and armeabi-v7a are targeted.</p>
<p>Once the ANDROID_NDK_ROOT is part of PATH, compile the package for Android CPU target: </p><pre class="fragment">cd $SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage
make cpu_android PLATFORM=$UDO_APP_ABI
</pre><p>The expected artifacts after compiling for Android CPU are</p><ul>
<li>SoftmaxUdoPackage/libs/$UDO_APP_ABI/libUdoSoftmaxUdoPackageImplCpu.so</li>
<li>SoftmaxUdoPackage/libs/$UDO_APP_ABI/libUdoSoftmaxUdoPackageReg.so</li>
<li>SoftmaxUdoPackage/libs/$UDO_APP_ABI/libc++_shared.so</li>
</ul>
<p><b>Note:</b> ndk-build attempts to build implementation libraries for both CPU and GPU runtime. This may result in compilation failures if OpenCL libraries cannot be found. This dependency can be worked around by removing GPU modules in SoftmaxUdoPackage/jni/Android.mk. Alternatively, GPU can be removed as a core type in Softmax.json during package generation in step 2.</p>
<p><b>Android GPU Runtime Compilation</b></p>
<p>Compilation for the Android GPU runtime uses the same Android NDK toolchain as the CPU. The remainder of the tutorial assumes ANDROID_NDK_ROOT has been set and added to PATH. Additionally, the CL_INCLUDE_PATH and CL_LIBRARY_PATH environment variables must be set to compile for Android GPU.</p>
<pre class="fragment">export CL_INCLUDE_PATH=&lt;path_to_cl_include&gt;
export CL_LIBRARY_PATH=&lt;path_to_libOpenCL.so&gt;
</pre><p><b>Note:</b> libOpenCL.so is platform dependent. It is the user's responsibility to obtain libOpenCL.so. libOpenCL.so and Open CL headers do not ship with the SNPE SDK.</p>
<p>With environment set, compile the package for Android GPU target: </p><pre class="fragment">cd $SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage
make gpu PLATFORM=$UDO_APP_ABI
</pre><p>The expected artifacts after compiling for Android GPU are</p><ul>
<li>SoftmaxUdoPackage/libs/$UDO_APP_ABI/libUdoSoftmaxUdoPackageImplGpu.so</li>
<li>SoftmaxUdoPackage/libs/$UDO_APP_ABI/libUdoSoftmaxUdoPackageReg.so</li>
<li>SoftmaxUdoPackage/libs/$UDO_APP_ABI/libOpenCL.so</li>
<li>SoftmaxUdoPackage/libs/$UDO_APP_ABI/libc++_shared.so</li>
</ul>
<p><b>Hexagon DSP Runtime Compilation</b></p>
<p>Compilation for the DSP runtime makes use of the make system. In order to build the DSP implementation libraries, Hexagon-SDK needs to be installed and set up. For details, follow the setup instructions on <code>$HEXAGON_SDK_ROOT/docs/readme.html</code> page, where <code>HEXAGON_SDK_ROOT</code> is the location of your Hexagon-SDK installation.</p>
<p><b>Note:</b> This SNPE release supports building UDO DSP implementation libraries using Hexagon-SDK 3.5.1/3.5.2.</p>
<p>Make sure that HEXAGON_SDK_ROOT, HEXAGON_TOOLS_ROOT and SDK_SETUP_ENV=Done is set. </p><pre class="fragment">export HEXAGON_SDK_ROOT=&lt;path to hexagon sdk installation&gt;
export HEXAGON_TOOLS_ROOT=$HEXAGON_SDK_ROOT/tools/HEXAGON_Tools/8.3.07
export SDK_SETUP_ENV=Done
</pre><p>With the environment set up, compile for DSP with the following: </p><pre class="fragment">cd $SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage
make dsp PLATFORM=$UDO_ABI
</pre><p>The expected artifacts after compiling for Hexagon DSP are</p><ul>
<li>SoftmaxUdoPackage/libs/dsp/libUdoSoftmaxUdoPackageImplDsp.so</li>
<li>SoftmaxUdoPackage/libs/$UDO_APP_ABI/libUdoSoftmaxUdoPackageReg.so</li>
</ul>
<p><b>Note:</b> For DSP, PLATFORM will only determine the ABI of the registration library.</p>
<h1><a class="anchor" id="inceptionv3_udo_setup"></a>
Setup Script</h1>
<p>The SNPE SDK provides an option to automatically perform steps of dlc conversion, package generation, package implementation, and package compilation for UDO as outlined in steps 1-4 above. The option is an extension of the <a class="el" href="tutorial_setup.html#tutorial_setup_inception_v3">Inception V3 setup script</a>. To enable Inception-V3 setup for UDO, run the script with the <b>--udo</b> or <b>-u</b> option.</p>
<pre class="fragment">usage: $SNPE_ROOT/models/inception_v3/scripts/setup_inceptionv3.py [-h] -a ASSETS_DIR [-d] [-r RUNTIME] [-u]

Prepares the inception_v3 assets for tutorial examples.

required arguments:
  -a ASSETS_DIR, --assets_dir ASSETS_DIR
                        directory containing the inception_v3 assets

optional arguments:
  -d, --download        Download inception_v3 assets to inception_v3 example
                        directory
  -r RUNTIME, --runtime RUNTIME
                        Choose a runtime to set up tutorial for. Choices: cpu,
                        gpu, dsp, aip, all. 'all' option is only supported
                        with --udo flag
  -u, --udo             Generate and compile a user-defined operation package
                        to be used with inception_v3. Softmax is simulated as
                        a UDO for this script.</pre><p>The --udo extension is compatible with options normally used by the setup_inceptionv3.py script. When the --udo option is enabled, the -r or --runtime option controls the runtime for the package implementation and compilation. Additionally, the --udo option supports use of an 'all' runtime option to create and compile the SoftmaxUdo Package for the CPU, GPU, and DSP/AIP runtimes. Selecting the 'aip' or 'dsp' runtime options additionally compiles x86 libraries in order to quantize the model. Selecting the 'cpu' runtime option compiles for both x86 and Android targets. Compilation for Android target will be skipped if ANDROID_NDK_ROOT is not set. If no runtime option is provided the package is compiled for the CPU runtime.</p>
<p>The command to use the setup script for UDO is: </p><pre class="fragment">python3 $SNPE_ROOT/models/inception_v3/scripts/setup_inceptionv3.py -a ~/tmpdir -d -u -r &lt;runtime_of_choice&gt;
</pre><p>For instance, to create and compile a package containing the libraries for all runtimes run: </p><pre class="fragment">python3 $SNPE_ROOT/models/inception_v3/scripts/setup_inceptionv3.py -a ~/tmpdir -d -u -r all
</pre><p>This will populate the artifacts in <a class="el" href="tutorial_inceptionv3_udo.html#inceptionv3_udo_compilation">Step 4</a>.</p>
<p><b>Note:</b> Setup script compiles for arm64-v8a platform architecture. To compile for different target set PLATFORM when using make. See <a class="el" href="tutorial_inceptionv3_udo.html#inceptionv3_udo_compilation">step 4</a> for more.</p>
<h1><a class="anchor" id="inceptionv3_udo_execution"></a>
Model Execution</h1>
<p><b>Execution using snpe-net-run</b></p>
<p>Executing Inception-V3 for UDO is largely the same as use of <a class="el" href="tutorial_inceptionv3.html#overview_native_app">snpe-net-run</a> without UDO.</p>
<p>The SNPE SDK provides Linux and Android binaries of <b>snpe-net-run</b> under</p><ul>
<li>$SNPE_ROOT/bin/x86_64-linux-clang</li>
<li>$SNPE_ROOT/bin/arm-android-clang6.0</li>
<li>$SNPE_ROOT/bin/aarch64-android-clang6.0</li>
<li>$SNPE_ROOT/bin/aarch64-linux-gcc4.9</li>
<li>$SNPE_ROOT/bin/arm-linux-gcc4.9sf</li>
<li>$SNPE_ROOT/bin/aarch64-oe-linux-gcc6.4</li>
<li>$SNPE_ROOT/bin/arm-oe-linux-gcc6.4hf</li>
</ul>
<p>For UDO, snpe-net-run consumes the registration library through the --udo_package_path option. LD_LIBRARY_PATH must also be updated to include the runtime-specific artifacts generated from package compilation.</p>
<p><b>x86 Host Execution</b></p>
<p>To execute the network on x86 host, run: </p><pre class="fragment">cd $SNPE_ROOT/models/inception_v3
export LD_LIBRARY_PATH = $LD_LIBRARY_PATH:$SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage/libs/x86/
snpe-net-run --container dlc/inception_v3_udo.dlc --input_list data/cropped/raw_list.txt --udo_package_path SoftmaxUdoPackage/libs/x86/libUdoSoftmaxUdoPackageReg.so
</pre><p><b>Android Target Execution</b></p>
<p>The tutorial for execution on Android targets will use the arm64-v8a architecture. This portion of the tutorial is generic to all runtimes (CPU, GPU, DSP).</p>
<pre class="fragment"># architecture: arm64-v8a - compiler: clang - STL: libc++
export SNPE_TARGET_ARCH=aarch64-android-clang6.0
export SNPE_TARGET_STL=libc++_shared.so
</pre><p>Then, push SNPE binaries and libraries to the target device: </p><pre class="fragment">adb shell "mkdir -p /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin"
adb shell "mkdir -p /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib"

adb push $SNPE_ROOT/lib/$SNPE_TARGET_ARCH/$SNPE_TARGET_STL \
      /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
adb push $SNPE_ROOT/lib/$SNPE_TARGET_ARCH/*.so \
      /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
adb push $SNPE_ROOT/bin/$SNPE_TARGET_ARCH/snpe-net-run \
      /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
</pre><p>Next, update environment variables on the target device to include the SNPE libraries and binaries: </p><pre class="fragment">adb shell
export SNPE_TARGET_ARCH=aarch64-android-clang6.0
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
export PATH=$PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
</pre><p>Lastly, push the Inception-V3 UDO model and input data to the device: </p><pre class="fragment">cd $SNPE_ROOT/models/inception_v3
mkdir data/rawfiles &amp;&amp; cp data/cropped/*.raw data/rawfiles/
adb shell "mkdir -p /data/local/tmp/inception_v3_udo"
adb push data/rawfiles /data/local/tmp/inception_v3_udo/cropped
adb push data/target_raw_list.txt /data/local/tmp/inception_v3_udo
adb push dlc/inception_v3_udo.dlc /data/local/tmp/inception_v3_udo
rm -rf data/rawfiles
</pre><p><b>Android CPU Execution</b></p>
<p>Once the model and data have been placed on the device, place the UDO libraries on the device: </p><pre class="fragment">cd $SNPE_ROOT/models/inception_v3
adb shell "mkdir -p /data/local/tmp/inception_v3_udo/cpu"
adb push SoftmaxUdoPackage/libs/arm64-v8a/libUdoSoftmaxUdoPackageImplCpu.so /data/local/tmp/inception_v3_udo/cpu
adb push SoftmaxUdoPackage/libs/arm64-v8a/libUdoSoftmaxUdoPackageReg.so /data/local/tmp/inception_v3_udo/cpu
adb push SoftmaxUdoPackage/libs/arm64-v8a/libc++_shared.so /data/local/tmp/inception_v3_udo/cpu
</pre><p>Now set required environment variables and run snpe-net-run on device: </p><pre class="fragment">adb shell
cd /data/local/tmp/inception_v3_udo/
export LD_LIBRARY_PATH=/data/local/tmp/inception_v3_udo/cpu/:$LD_LIBRARY_PATH
snpe-net-run --container inception_v3_udo.dlc --input_list target_raw_list.txt --udo_package_path cpu/libUdoSoftmaxUdoPackageReg.so
</pre><p><b>Android GPU Execution</b></p>
<p>The procedure for execution using the GPU runtime is largely similar to execution procedure for CPU. First, use the following to place the UDO libraries on device: </p><pre class="fragment">cd $SNPE_ROOT/models/inception_v3
adb shell "mkdir -p /data/local/tmp/inception_v3_udo/gpu"
adb push SoftmaxUdoPackage/libs/arm64-v8a/libUdoSoftmaxUdoPackageImplGpu.so /data/local/tmp/inception_v3_udo/gpu
adb push SoftmaxUdoPackage/libs/arm64-v8a/libUdoSoftmaxUdoPackageReg.so /data/local/tmp/inception_v3_udo/gpu
adb push SoftmaxUdoPackage/libs/arm64-v8a/libc++_shared.so /data/local/tmp/inception_v3_udo/gpu
adb push SoftmaxUdoPackage/libs/arm64-v8a/libOpenCL.so /data/local/tmp/inception_v3_udo/gpu
</pre><p>Now set required environment variables and run snpe-net-run to device: </p><pre class="fragment">adb shell
cd /data/local/tmp/inception_v3_udo/
export LD_LIBRARY_PATH=data/local/tmp/inception_v3_udo/gpu/:$LD_LIBRARY_PATH
snpe-net-run --container inception_v3_udo.dlc --input_list target_raw_list.txt --udo_package_path gpu/libUdoSoftmaxUdoPackageReg.so --use_gpu
</pre><p><b>Hexagon DSP Execution</b></p>
<p>The procedure for execution on device for DSP is largely the same as CPU and GPU. However, the DSP runtime requires quantized network parameters. While DSP allows unquantized DLCs, it is generally recommended to quantize DLCs for improved performance. The tutorial will use a quantized DLC as an illustrative example. Quantizing the DLC requires the <b>snpe-dlc-quantize</b> tool.</p>
<p>To quantize the DLC for use on DSP: </p><pre class="fragment">cd $SNPE_ROOT/models/inception_v3/
snpe-dlc-quantize --input_dlc dlc/inception_v3_udo.dlc --input_list data/cropped/raw_list.txt --udo_package_path SoftmaxUdoPackage/libs/x86-64_linux_clang/libUdoSoftmaxUdoPackageReg.so --output_dlc dlc/inception_v3_udo_quantized.dlc
</pre><p>For more information on snpe-dlc-quantize visit <a class="el" href="quantized_models.html#overview_quantize">quantization</a>. For information on UDO-specific quantization visit <a class="el" href="preparing_model_with_udo.html#quantize_network_with_udo">Quantizing a DLC with UDO</a>. For information on DSP/AIP runtime visit <a class="el" href="dsp_runtime.html">DSP Runtime</a> or <a class="el" href="aip_runtime.html">AIP Runtime</a>.</p>
<p>Now push the quantized model to device: </p><pre class="fragment">adb push dlc/inception_v3_udo_quantized.dlc /data/local/tmp/inception_v3_udo
</pre><p><b>Note:</b> Please refer to <a class="el" href="tutorial_inceptionv3_udo_dsp.html">UDO DSP for Quantized DLC</a> tutorial for executing on the DSP runtime using quantized dlc.</p>
<p>Before executing on the DSP, push the SNPE libraries for DSP to device: </p><pre class="fragment">adb shell "mkdir -p /data/local/tmp/snpeexample/dsp/lib"
adb push $SNPE_ROOT/lib/dsp/*.so \
      /data/local/tmp/snpeexample/dsp/lib
</pre><p>Now push DSP-specific UDO libraries to device: </p><pre class="fragment">cd $SNPE_ROOT/models/inception_v3
adb shell "mkdir -p /data/local/tmp/inception_v3_udo/dsp"
adb push SoftmaxUdoPackage/libs/dsp/*.so /data/local/tmp/inception_v3_udo/dsp
adb push SoftmaxUdoPackage/libs/arm64-v8a/libUdoSoftmaxUdoPackageReg.so /data/local/tmp/inception_v3_udo/dsp # Pushes reg lib
</pre><p>Then set required environment variables and run snpe-net-run on device: </p><pre class="fragment">adb shell
cd /data/local/tmp/inception_v3_udo/
export LD_LIBRARY_PATH=/data/local/tmp/inception_v3_udo/dsp/:$LD_LIBRARY_PATH
export ADSP_LIBRARY_PATH="/data/local/tmp/inception_v3_udo/dsp/;/data/local/tmp/snpeexample/dsp/lib;/system/lib/rfsa/adsp;/system/vendor/lib/rfsa/adsp;/dsp"
snpe-net-run --container inception_v3_udo_quantized.dlc --input_list target_raw_list.txt --udo_package_path dsp/libUdoSoftmaxUdoPackageReg.so --use_dsp
</pre><p><b>AIP Execution</b></p>
<p>Because UDOs are not supported on the HTA hardware, executing on the AIP runtime defaults to the DSP UDO implementations. HTA hardware runs exclusively on quantized models and therefore as with the DSP runtime, a quantized model will be used. The command to quantize the DLC for AIP is: </p><pre class="fragment">cd $SNPE_ROOT/models/inception_v3/
snpe-dlc-quantize --input_dlc dlc/inception_v3_udo.dlc --input_list data/cropped/raw_list.txt --udo_package_path SoftmaxUdoPackage/libs/x86-64_linux_clang/libUdoSoftmaxUdoPackageReg.so --output_dlc dlc/inception_v3_udo_quantized.dlc --enable_hta
</pre><p>Now push the quantized model to device: </p><pre class="fragment">adb push dlc/inception_v3_udo_quantized.dlc /data/local/tmp/inception_v3_udo
</pre><p>Before executing using the AIP runtime, push the SNPE libraries for DSP to device with these commands: </p><pre class="fragment">adb shell "mkdir -p /data/local/tmp/snpeexample/dsp/lib"
adb push $SNPE_ROOT/lib/dsp/*.so \
      /data/local/tmp/snpeexample/dsp/lib
</pre><p>Now push DSP-specific UDO libraries to device: </p><pre class="fragment">cd $SNPE_ROOT/models/inception_v3
adb shell "mkdir -p /data/local/tmp/inception_v3_udo/dsp"
adb push SoftmaxUdoPackage/libs/dsp/*.so /data/local/tmp/inception_v3_udo/dsp
adb push SoftmaxUdoPackage/libs/arm64-v8a/libUdoSoftmaxUdoPackageReg.so /data/local/tmp/inception_v3_udo/dsp # Pushes reg lib
</pre><p>Then set required environment variables and run snpe-net-run on device: </p><pre class="fragment">adb shell
cd /data/local/tmp/inception_v3_udo/
export LD_LIBRARY_PATH=/data/local/tmp/inception_v3_udo/dsp/:$LD_LIBRARY_PATH
export ADSP_LIBRARY_PATH="/data/local/tmp/inception_v3_udo/dsp/;/data/local/tmp/snpeexample/dsp/lib;/system/lib/rfsa/adsp;/system/vendor/lib/rfsa/adsp;/dsp"
snpe-net-run --container inception_v3_udo_quantized.dlc --input_list target_raw_list.txt --udo_package_path dsp/libUdoSoftmaxUdoPackageReg.so --use_aip
</pre><p><b>Integration with Android APK</b></p>
<p>This portion of the tutorial outlines how to integrate SNPE UDO libraries and Java API for package registration into an example application. Generally, for native shared libraries to be discoverable by the application they must be placed in the project under </p><pre class="fragment">&lt;project&gt;/app/src/main/jniLibs/&lt;platform_abi&gt;
</pre><p>Once the libraries are accessible by the application, the registration library can be registered using the provided <a class="el" href="running_model_with_udo.html#executing_nets_with_udo">Java API</a>. This process will be replicated with the example <a class="el" href="android_tutorial.html#android_sample_applications">Image Classifiers</a> application. The following assumes that the rest of the example application setup has been followed. The tutorial will issue instructions for platforms with arm64-v8a ABI.</p>
<p>First, create the neccessary directories to contain the UDO libraries. The following steps will populate all runtime implementation libraries. </p><pre class="fragment">mkdir app/src/main/jniLibs/
cp -a $SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage/libs/arm64-v8a/ app/src/main/jniLibs/
</pre><p>If DSP is to be used as the runtime, copy the implementation library with the following: </p><pre class="fragment">cp $SNPE_ROOT/models/inception_v3/SoftmaxUdoPackage/libs/dsp/*.so app/src/main/jniLibs/arm64-v8a/
</pre><p>If not already done, running <b>setup_inceptionv3.sh</b> will add the Inception-V3 model enabled with UDO to the project. </p><pre class="fragment">bash ./setup_inceptionv3.sh
</pre><p>Now the Java API can be registered. Edit the file $SNPE_ROOT/examples/android/image-classifiers/app/src/main /java/com/qualcomm/qti/snpe/imageclassifiers/tasks/LoadNetworkTask.java</p>
<p>To contain this line </p><pre class="fragment">@Override
    protected NeuralNetwork doInBackground(File... params) {
        NeuralNetwork network = null;
        try {
            SNPE.addOpPackage(mApplication,"libUdoSoftmaxUdoPackageReg.so"); // Add this line to register package
            final SNPE.NeuralNetworkBuilder builder = new SNPE.NeuralNetworkBuilder(mApplication)
        ...
</pre><p>Now the APK can be built and exercised </p><pre class="fragment">./gradlew assembleDebug
</pre> </div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 -->
<!-- start footer part -->
<div id="nav-path" class="navpath" font-size:small;><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
      <p align="right">
        80-NL315-14 A <br>
        MAY CONTAIN U.S. AND INTERNATIONAL EXPORT CONTROLLED INFORMATION
        <!--If the Controlled Distribution statement is to be included, uncomment below:-->
        <!--<b>Controlled Distribution - DO NOT COPY</b>-->
        <img class="footer" width:5%; alt="QTI Logo" src="images/QTI_Logo.png" />
      </p>
    </li>
  </ul>
</div>
</body>
</html>
